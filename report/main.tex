\documentclass[10pt]{article}

\usepackage[margin=1in]{geometry}
\usepackage{times}
\usepackage{amsmath,amssymb}
\usepackage{graphicx}
\usepackage{subcaption}
\usepackage{booktabs}
\usepackage{multirow}
\usepackage{url}
\usepackage[hidelinks]{hyperref}

\setlength{\parskip}{0.5ex}
\setlength{\parindent}{0pt}

\title{Image--Text Retrieval on Flickr30k using BLIP-2:\\
Pretrained, Baseline, and Improved Fine-Tuning}

\author{Mukhil}

\date{}

\begin{document}
\maketitle

% -------------------------------------------------------------
% Figure 1 -- task illustration (matches template)
\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/fig_sample_image_24132}
\caption{Illustration of the Flickr30k image--text retrieval task.
Given an image (or a caption), the system ranks all captions (or images) and should place the correct match near the top.}
\label{fig:task}
\end{figure}

% -------------------------------------------------------------
\section{Task}

The goal of this project is to study \textbf{image--text retrieval} on the Flickr30k dataset using a modern vision--language model.
The retrieval task is bi-directional:

\textbf{Image-to-text (I2T):}
given a query image, retrieve the correct caption from a large pool of candidate captions.

\textbf{Text-to-image (T2I):}
given a query caption, retrieve the corresponding image from the test set.

We follow the standard evaluation protocol used in the literature.
For each query we sort all candidates by cosine similarity in a shared embedding space and compute Recall@K (R@K) for $K \in \{1, 5, 10\}$.
R@K measures the percentage of queries whose correct item appears in the top $K$ retrieved results.
We report R@K for both I2T and T2I directions.

The project focuses on three concrete questions:

\begin{enumerate}
    \item How well does a strong pretrained BLIP-2 model perform on Flickr30k retrieval when used ``as is''?
    \item How much can performance be improved by training a simple linear projection head on Flickr30k?
    \item Can we obtain further gains by replacing the linear projection with a deeper MLP projection head?
\end{enumerate}

These questions allow us to replicate and interpret a state-of-the-art (SOTA) method while also exploring a simple but meaningful architectural improvement.

% -------------------------------------------------------------
\section{Related Work}

\subsection*{CLIP, ALBEF, BLIP, X-VLM, BLIP-2}

\textbf{CLIP}~\cite{radford2021clip} introduced large-scale contrastive pretraining on 400M image--text pairs.
A dual encoder maps images and texts into a shared space where cosine similarity supports zero-shot retrieval and classification.
CLIP is strong but mainly targets global alignment; fine-grained caption grounding is more limited.

\textbf{ALBEF}~\cite{li2021albef} proposed ``Align Before Fuse'': a vision--language transformer trained with momentum distillation.
It combines contrastive learning and cross-attention to obtain tight alignment between image and text tokens, achieving strong retrieval performance on MS-COCO and Flickr30k.

\textbf{BLIP}~\cite{li2022blip} (Bootstrapping Language--Image Pretraining) unified captioning and retrieval with caption bootstrapping and multi-task pretraining.
It improved both caption quality and retrieval metrics over earlier models.

\textbf{X-VLM}~\cite{zeng2022xvlm} further scaled up vision--language pretraining across multiple tasks (detection, captioning, retrieval) using a single unified model.
X-VLM reached very strong retrieval performance but requires heavy training and careful engineering.

\textbf{BLIP-2}~\cite{li2023blip2} introduced a new architecture that freezes a powerful vision encoder and a large language model while training an intermediate Q-Former to connect them.
This design achieves excellent performance on captioning, VQA, and retrieval with relatively modest additional training, and is particularly attractive when we want to adapt a strong backbone to new tasks.

\subsection*{Why BLIP-2 as SOTA}

We select BLIP-2 as the SOTA method for this project because it outperforms prior image--text models such as CLIP, ALBEF, BLIP, and X-VLM on retrieval benchmarks like Flickr30k and MS-COCO.
BLIP-2 introduces a Q-Former that efficiently bridges frozen vision encoders with large language models, enabling strong cross-modal alignment with low additional training.

Compared to:
\begin{itemize}
    \item \textbf{CLIP}: strong global alignment but weaker fine-grained caption grounding,
    \item \textbf{ALBEF}: learns cross-attention but requires full end-to-end training,
    \item \textbf{BLIP}: earlier architecture with slightly lower retrieval numbers,
    \item \textbf{X-VLM}: strong retrieval but heavier and more complex to train,
\end{itemize}
BLIP-2 provides the best trade-off between accuracy, simplicity, and extensibility.
For a course project, it offers both a strong baseline and enough architectural structure to explore improvements.

Table~\ref{tab:sota} summarizes the reported image-to-text results on Flickr30k from these methods.

\begin{table}[h]
\centering
\caption{Representative Flickr30k image-to-text retrieval results reported in the literature (R@1 and R@10, in \%).}
\label{tab:sota}
\begin{tabular}{lccc}
\toprule
Method & Dataset & R@1 & R@10 \\
\midrule
CLIP (2021) & Flickr30k & 88.0 & 99.1 \\
ALBEF (2021) & Flickr30k & 95.9 & 99.8 \\
BLIP (2022) & Flickr30k & 96.1 & 99.8 \\
X-VLM (2022) & Flickr30k & 96.4 & 99.9 \\
BLIP-2 (2023) & Flickr30k & \textbf{96.7} & \textbf{99.9} \\
\bottomrule
\end{tabular}
\end{table}

We treat these numbers as the SOTA reference and then study how well a BLIP-2-style model performs under our experimental design.

% -------------------------------------------------------------
\section{Approach}

\subsection*{High-Level Architecture}

We use the \texttt{Salesforce/blip2-flan-t5-xl} checkpoint from HuggingFace as a frozen backbone.
A Vision Transformer (ViT-G) encodes each image into a sequence of visual tokens.
A Q-Former with a small number of learned query tokens attends to these tokens and produces a compact image representation.
On the text side, a T5 encoder processes the caption into a sequence of textual embeddings.

We do not modify or fine-tune these large components.
Instead, we add small projection heads on top of the image and text embeddings and train only these heads using Flickr30k.
This setup lets us isolate the role of the alignment layers and makes it easy to compare different projection-head designs.

\subsection*{Projection Heads}

\textbf{Baseline head.}
The baseline projection head uses a linear layer for images and a separate linear layer for text.
Each linear layer maps its input embedding to a 768-dimensional shared space.
We then apply L2 normalization and a learned temperature parameter.
Cosine similarity in this space is used for retrieval.

\textbf{Improved head.}
The improved projection head replaces each single linear layer with a two-layer MLP:
a linear layer, GELU activation, and a second linear layer, again mapping to 768 dimensions.
This deeper non-linear mapping gives the model more capacity to align the image and text manifolds while still keeping the trainable parameter count modest.

\subsection*{Training Objective}

For a batch of $B$ paired image and caption embeddings we form a similarity matrix $S \in \mathbb{R}^{B \times B}$ using scaled cosine similarity between all image and text embeddings.
We use a symmetric contrastive loss:
for each image we treat its own caption as the positive example and all other captions in the batch as negatives (image-to-text direction), and we do the same in the text-to-image direction.
The final loss is the average of both directions.

\subsection*{Training Strategy}

To focus on the effect of the projection heads and to allow repeated experiments, we train on a randomly selected subset of 5{,}000 image--caption pairs from the Flickr30k training split.
The baseline model is trained for 5 epochs on this subset.
The improved model is trained on the same subset with a slightly longer schedule (8 epochs) and a tuned learning rate.
All other hyperparameters (optimizer, batch size, temperature initialization) are kept consistent between the two runs so that differences in performance can be attributed primarily to the projection head.

% -------------------------------------------------------------
\section{Dataset}

We use the Flickr30k dataset~\cite{young2014flickr30k} through the modern HuggingFace Parquet release \texttt{lmms-lab/flickr30k}, which stores images and captions directly in Parquet files and provides train/validation/test splits.

The full dataset contains 31,783 images.
In the variant used here, each example consists of one image and one caption.
Images are preprocessed using the BLIP-2 image processor (resize, crop, normalization), and captions are tokenized with the T5 tokenizer.

For training we sample 5{,}000 image--caption pairs from the training split.
For evaluation we use the entire official test split.
This design makes the experiments reproducible while still keeping the retrieval problem realistic: for each test query we rank all thousands of candidates.

% -------------------------------------------------------------
\section{Results}

\subsection*{Overall Recall@K}

Table~\ref{tab:overall} summarizes the quantitative results for all three models:
the frozen pretrained BLIP-2 encoder (\emph{Pretrained}),
the baseline linear projection head (\emph{Baseline}), and
the improved 2-layer MLP projection head (\emph{Improved}).
All numbers are reported as percentages.

\begin{table}[h]
\centering
\caption{Overall Flickr30k retrieval results (R@K in \%).}
\label{tab:overall}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} &
\multicolumn{3}{c}{Image $\rightarrow$ Text} &
\multicolumn{3}{c}{Text $\rightarrow$ Image} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
Pretrained & 0.00 & 0.01 & 0.03 & 0.00 & 0.02 & 0.04 \\
Baseline (5k) & 20.67 & 41.17 & 51.10 & 20.07 & 40.47 & 50.96 \\
Improved (5k + 8 epochs) & \textbf{25.27} & \textbf{46.62} & \textbf{56.16} &
\textbf{26.10} & \textbf{48.08} & \textbf{58.26} \\
\bottomrule
\end{tabular}
\end{table}

The pretrained model behaves almost like a random retrieval system on Flickr30k, with Recall@10 near zero.
Once we train projection heads on the 5k subset, performance jumps to around 41--51\% R@5/R@10 in both directions.
Replacing the linear projection with a 2-layer MLP yields consistent gains of about 4--6 percentage points across all metrics.
These results show that even when the large encoders are frozen, well-designed projection heads can substantially improve retrieval quality.

\subsection*{Comparison to SOTA BLIP-2 Numbers}

Table~\ref{tab:replication} contextualizes our results with respect to the BLIP-2 paper.
The paper reports very high scores when BLIP-2 is fully trained with its complete recipe.
Our variant keeps the backbone frozen and focuses on the projection heads.

\begin{table}[h]
\centering
\caption{BLIP-2 paper results vs.\ our Flickr30k retrieval results (R@K in \%).}
\label{tab:replication}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} &
\multicolumn{3}{c}{Image $\rightarrow$ Text} &
\multicolumn{3}{c}{Text $\rightarrow$ Image} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
BLIP-2 (paper, full training) & 96.7 & -- & 99.9 & 95.0+ & -- & 99.0+ \\
Pretrained (ours) & 0.00 & 0.01 & 0.03 & 0.00 & 0.02 & 0.04 \\
Baseline (5k) & 20.67 & 41.17 & 51.10 & 20.07 & 40.47 & 50.96 \\
Improved (5k + 8 epochs) & 25.27 & 46.62 & 56.16 & 26.10 & 48.08 & 58.26 \\
\bottomrule
\end{tabular}
\end{table}

Our absolute R@K values are much lower than the fully trained BLIP-2 model, which is expected because we intentionally restrict training to alignment layers on a smaller subset.
However, this controlled setting is ideal for studying the effect of different projection heads:
the relative improvement from baseline to improved model is clear and consistent, and the experiments remain fully reproducible.

\subsection*{Performance Discussion}

The transition from the pretrained model to the baseline model shows that Flickr30k retrieval is not solved by simply using a generic BLIP-2 checkpoint.
The frozen encoders need task-specific alignment to map images and captions into a useful shared space.
Training linear projection heads on just 5k pairs already moves the system from essentially zero R@K to around 51\% R@10.

The improved projection head further boosts performance.
By allowing a non-linear transformation of the embeddings, the 2-layer MLP can correct systematic mismatches between the visual and textual spaces.
The gains are especially clear at R@1, where going from roughly 20\% to 25\% means that one in four queries now retrieves the correct item at the very top of the ranked list.

\subsection*{Recall@K Plot}

Figure~\ref{fig:recall} visualizes all six metrics (I2T/T2I, R@1/5/10) for the three models.

\begin{figure}[h]
\centering
\includegraphics[width=0.95\linewidth]{figures/fig_recall_comparison}
\caption{Recall@K comparison across pretrained, baseline, and improved models.
Each group of bars corresponds to a particular metric.
The pretrained model is almost flat at zero.
Fine-tuning projection heads yields a large jump in performance, and the improved MLP head consistently outperforms the baseline.}
\label{fig:recall}
\end{figure}

% -------------------------------------------------------------
\section{Possible Improvements and Results}

\subsection*{Projection-Head Improvement}

Table~\ref{tab:improvement} focuses specifically on the baseline vs.\ improved models.
Here we see the effect of the projection-head design in isolation.

\begin{table}[h]
\centering
\caption{Baseline vs.\ improved projection head (R@K in \%).}
\label{tab:improvement}
\begin{tabular}{lcccccc}
\toprule
\multirow{2}{*}{Model} &
\multicolumn{3}{c}{Image $\rightarrow$ Text} &
\multicolumn{3}{c}{Text $\rightarrow$ Image} \\
\cmidrule(lr){2-4} \cmidrule(lr){5-7}
& R@1 & R@5 & R@10 & R@1 & R@5 & R@10 \\
\midrule
Baseline (linear head) & 20.67 & 41.17 & 51.10 & 20.07 & 40.47 & 50.96 \\
Improved (2-layer MLP head) & \textbf{25.27} & \textbf{46.62} & \textbf{56.16} &
\textbf{26.10} & \textbf{48.08} & \textbf{58.26} \\
\bottomrule
\end{tabular}
\end{table}

The improved model gains roughly 4.6\% absolute at R@1 for both I2T and T2I, and around 5\% at R@10.
These consistent improvements demonstrate that a slightly deeper projection head is enough to produce more discriminative shared embeddings, even when the rest of the BLIP-2 model is frozen.

\subsection*{Embedding Visualization (PCA)}

To make the effect of training visually intuitive, we project both image and caption embeddings into 2D using PCA for a subset of test examples.

\begin{figure}[h]
\centering
\begin{subfigure}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/fig_pca_pretrained}
\caption{Pretrained model.}
\label{fig:pca_pre}
\end{subfigure}
\hfill
\begin{subfigure}[b]{0.45\linewidth}
\centering
\includegraphics[width=\linewidth]{figures/fig_pca_trained}
\caption{Improved model.}
\label{fig:pca_imp}
\end{subfigure}
\caption{PCA projection of image (blue) and caption (orange) embeddings on a subset of the Flickr30k test split.
Before training (left), the two modalities form diffuse, weakly aligned clouds.
After training the improved projection head (right), the points become more compact and interleaved, indicating stronger cross-modal alignment.}
\label{fig:pca}
\end{figure}

Even without technical background, one can see that in the pretrained case the blue and orange dots are more scattered, while after training they overlap more tightly.
This visual pattern matches the numerical improvements in Recall@K.

\subsection*{Qualitative Retrieval Example}

For a more concrete intuition, we inspect a specific test image (Figure~\ref{fig:sample}) and compare the retrieved captions.
The reference captions all describe a concert scene with a band performing on stage in front of an audience.

\begin{figure}[h]
\centering
\includegraphics[width=0.45\linewidth]{figures/fig_sample_image_24132}
\caption{Sample Flickr30k image used for qualitative analysis.
The ground-truth captions mention a band or performers on stage at a concert.}
\label{fig:sample}
\end{figure}

The pretrained model tends to retrieve captions that are clearly off-topic (e.g., markets, beaches, cooking), reflecting its near-zero Recall@K.
The fine-tuned models perform much better.
Tables~\ref{tab:qual_baseline} and~\ref{tab:qual_improved} show the top-5 captions retrieved by the baseline and improved models for this image.

\begin{table}[h]
\centering
\caption{Baseline model: top-5 retrieved captions for the concert image.}
\label{tab:qual_baseline}
\begin{tabular}{p{0.95\linewidth}}
\toprule
Baseline top-5 captions \\
\midrule
1.\ A group of musicians are performing on a stage in front of a crowd. \\
2.\ On stage photo of small band performing for theater audience. \\
3.\ A band on stage performing in front of a crowd. \\
4.\ A band plays on stage in front of an audience. \\
5.\ A band playing on stage for a crowd. \\
\bottomrule
\end{tabular}
\end{table}

\begin{table}[h]
\centering
\caption{Improved model: top-5 retrieved captions for the same image.}
\label{tab:qual_improved}
\begin{tabular}{p{0.95\linewidth}}
\toprule
Improved top-5 captions \\
\midrule
1.\ A band with spotlight on the guitarist plays on stage to a dark crowd. \\
2.\ A large group of people stand at a concert with sound equipment on a stage to the right. \\
3.\ A group of musicians are performing on a stage in front of a crowd. \\
4.\ Several people watch a female rock band performing on a stage full of yellow banners. \\
5.\ On a stage, there is a band playing guitars and singing, while the lights behind them flash and show a large star. \\
\bottomrule
\end{tabular}
\end{table}

Both models correctly recognize that the image is a concert scene.
However, the improved model produces richer and more detailed descriptions (spotlight, dark crowd, sound equipment, star-shaped background).
This example illustrates the qualitative impact of the improved projection head.

\subsection*{Discussion, Limitations, and Future Work}

The experiments highlight three main observations.
First, strong vision--language backbones still need task-specific alignment layers for challenging retrieval benchmarks like Flickr30k.
Second, even when only small projection heads are trained, a modern BLIP-2 backbone can achieve respectable retrieval performance.
Third, adding depth to the projection head systematically improves both metrics and qualitative behavior.

There are still limitations.
We focus on a selective fine-tuning scheme where the backbone remains frozen; fully updating the Q-Former and encoders might produce higher absolute R@K values.
We also train on a curated 5k subset rather than the entire training set, which simplifies repeated experimentation but leaves potential performance on the table.

Future work could fine-tune more of BLIP-2 end-to-end, train on the full Flickr30k training set, explore cross-attention-based alignment instead of separate projection heads, and incorporate hard-negative mining to further sharpen the contrastive learning signal.

% -------------------------------------------------------------
\section{Code Repository}

All code, the Colab notebook, and saved metrics are available at:

\medskip
\noindent
\textbf{GitHub:} \url{https://github.com/mukhilDS/Vision-Language_Final_project}

\medskip
The repository includes:
\begin{itemize}
    \item \texttt{notebook/flickr30k\_final\_project.ipynb} (main notebook).
    \item \texttt{outputs/pretrained\_eval/metrics.json} (pretrained evaluation).
    \item \texttt{outputs/baseline\_run/metrics\_finetuned.json} (baseline results).
    \item \texttt{outputs/improved\_run/metrics\_improved.json} (improved results).
    \item \texttt{figures/} (all plots and qualitative figures used in this report).
\end{itemize}

% -------------------------------------------------------------
\begin{thebibliography}{9}

\bibitem{radford2021clip}
A.~Radford et al.
\newblock Learning Transferable Visual Models from Natural Language Supervision.
\newblock In \emph{ICML}, 2021.

\bibitem{li2021albef}
J.~Li et al.
\newblock Align Before Fuse: Vision and Language Representation Learning with Momentum Distillation.
\newblock In \emph{NeurIPS}, 2021.

\bibitem{li2022blip}
J.~Li et al.
\newblock BLIP: Bootstrapping Language-Image Pre-training for Unified Vision-Language Understanding and Generation.
\newblock In \emph{ICML}, 2022.

\bibitem{zeng2022xvlm}
Y.~Zeng et al.
\newblock X-VLM: Unified Model for Cross-Modal Pre-Training.
\newblock In \emph{NeurIPS}, 2022.

\bibitem{li2023blip2}
J.~Li et al.
\newblock BLIP-2: Bootstrapped Language-Image Pre-training with Frozen Image Encoders and Large Language Models.
\newblock In \emph{ICML}, 2023.

\bibitem{young2014flickr30k}
P.~Young et al.
\newblock From Image Descriptions to Visual Denotations: New Similarity Metrics for Semantic Inference over Event Descriptions.
\newblock \emph{Transactions of the ACL}, 2014. (Introduces the Flickr30k dataset.)

\end{thebibliography}

\end{document}
